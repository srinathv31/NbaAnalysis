{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/srinath/Documents/NbaAnalysis/getdata.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 186>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=180'>181</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=183'>184</a>\u001b[0m \u001b[39m######################################################################################\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=184'>185</a>\u001b[0m \u001b[39m#MAIN \u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=185'>186</a>\u001b[0m players_general_info \u001b[39m=\u001b[39m player_basic_info() \u001b[39m# call function that scrapes general info\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=186'>187</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGeneral info/player url loaded...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=187'>188</a>\u001b[0m players_details_info_list \u001b[39m=\u001b[39m []\n",
      "\u001b[1;32m/Users/srinath/Documents/NbaAnalysis/getdata.ipynb Cell 1'\u001b[0m in \u001b[0;36mplayer_basic_info\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m letter \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mascii_lowercase:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=15'>16</a>\u001b[0m     page_request \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(base_url \u001b[39m+\u001b[39m letter)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=16'>17</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(page_request\u001b[39m.\u001b[39;49mtext,\u001b[39m\"\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=17'>18</a>\u001b[0m     table \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/srinath/Documents/NbaAnalysis/getdata.ipynb#ch0000000?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m table:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/bs4/__init__.py:243\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=240'>241</a>\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=241'>242</a>\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=242'>243</a>\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=243'>244</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=244'>245</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=245'>246</a>\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=247'>248</a>\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=248'>249</a>\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=249'>250</a>\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/lib/python3.9/site-packages/bs4/__init__.py?line=250'>251</a>\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import sys\n",
    "import string\n",
    "import requests\n",
    "import datetime\n",
    "import progressbar\n",
    "import time\n",
    "import re\n",
    "\n",
    "def player_basic_info():\n",
    "    players = []\n",
    "    base_url = 'http://www.basketball-reference.com/players/'\n",
    "    for letter in string.ascii_lowercase:\n",
    "        page_request = requests.get(base_url + letter)\n",
    "        soup = BeautifulSoup(page_request.text,\"lxml\")\n",
    "        table = soup.find('table')\n",
    "        if table:\n",
    "            table_body = table.find('tbody')\n",
    "            for row in table_body.findAll('tr'):\n",
    "                player_url = row.find('a')\n",
    "                player_names = player_url.text\n",
    "                player_pages = player_url['href']\n",
    "                cells = row.findAll('td') # all data for all players uniform across database\n",
    "                active_from = int(cells[0].text)\n",
    "                active_to = int(cells[1].text)\n",
    "                position = cells[2].text\n",
    "                height = cells[3].text\n",
    "                weight = cells[4].text\n",
    "                birth_date = cells[5].text\n",
    "                college = cells[6].text    \n",
    "                player_entry = {'url': player_pages,\n",
    "                                'name': player_names,\n",
    "                                'active_from': active_from,\n",
    "                                'active_to': active_to,\n",
    "                                'position': position,\n",
    "                                'college': college,\n",
    "                                'height': height,\n",
    "                                'weight': weight,\n",
    "                                'birth_date': birth_date}\n",
    "                players.append(player_entry)\n",
    "    return pd.DataFrame(players)\n",
    "\n",
    "def player_info(url):\n",
    "    #define all quantites\n",
    "    fgpct = None\n",
    "    games = None\n",
    "    ppg = None\n",
    "    ft = None\n",
    "    fgpg = None\n",
    "    fgapg = None\n",
    "    ftpg = None\n",
    "    ftapg = None\n",
    "    _3ptpg = None\n",
    "    _3ptapg = None\n",
    "    _3ptpct = None\n",
    "    efgpct = None\n",
    "    NCAA_fgpct = None\n",
    "    NCAA_games = None\n",
    "    NCAA_ppg = None\n",
    "    NCAA_ft = None\n",
    "    NCAA_fgpg = None\n",
    "    NCAA_fgapg = None\n",
    "    NCAA_ftpg = None\n",
    "    NCAA_ftapg = None\n",
    "    NCAA__3ptpg = None\n",
    "    NCAA__3ptapg = None\n",
    "    NCAA__3ptpct = None\n",
    "    NCAA_efgpct = None\n",
    "    #print('url = ' + str('http://www.basketball-reference.com' + str(url)))\n",
    "    page_request = requests.get('http://www.basketball-reference.com' + str(url))\n",
    "    soup = BeautifulSoup(page_request.text,\"lxml\")\n",
    "    table = soup.find('table') #the first table is luckily the per game stats\n",
    "    if table:\n",
    "        table_foot = table.find('tfoot')\n",
    "        for row in table_foot.findAll('tr'):\n",
    "            cells  = row.findAll('td')\n",
    "            playerData = str(cells) #the indexes are not uniform across the database\n",
    "            #games = re.search(r'data-stat=\"g\">(.*?)</td>', playerData).group(1) # don't need\n",
    "            fgpct = re.search(r'data-stat=\"fg_pct\">(.*?)</td>', playerData).group(1)\n",
    "            games = re.search(r'data-stat=\"g\">(.*?)</td>', playerData).group(1)\n",
    "            ppg = re.search(r'data-stat=\"pts_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            ft = re.search(r'data-stat=\"ft_pct\">(.*?)</td>', playerData).group(1)\n",
    "            fgpg = re.search(r'data-stat=\"fg_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            fgapg = re.search(r'data-stat=\"fga_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            ftpg = re.search(r'data-stat=\"ft_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            ftapg = re.search(r'data-stat=\"fta_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            if re.search(r'data-stat=\"fg3_per_g\">(.*?)</td>', playerData) != None:\n",
    "                _3ptpg = re.search(r'data-stat=\"fg3_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            else:\n",
    "                _3ptpg = None\n",
    "            if re.search(r'data-stat=\"fg3a_per_g\">(.*?)</td>', playerData) != None:\n",
    "                _3ptapg = re.search(r'data-stat=\"fg3a_per_g\">(.*?)</td>', playerData).group(1)\n",
    "            else:\n",
    "                _3ptapg = None\n",
    "            if re.search(r'data-stat=\"fg3_pct\">(.*?)</td>', playerData) != None:\n",
    "                _3ptpct = re.search(r'data-stat=\"fg3_pct\">(.*?)</td>', playerData).group(1)\t\n",
    "            else:\n",
    "                _3ptpct = None\n",
    "            if re.search(r'data-stat=\"efg_pct\">(.*?)</td>', playerData) != None:\n",
    "                efgpct = re.search(r'data-stat=\"efg_pct\">(.*?)</td>', playerData).group(1)\n",
    "            else:\n",
    "                efgpct = None\n",
    "            break  #bad but I want the structure to remain the same in case I want more data outside overall stats\n",
    "\n",
    "    college_url = get_player_college_url(url)\n",
    "    if(college_url != None):\n",
    "        page_request_cbb = requests.get(college_url)\n",
    "        soupy = BeautifulSoup(page_request_cbb.text,'lxml')\n",
    "        table_cbb = soupy.find('table')\n",
    "        if table_cbb:\n",
    "            table_foot = table_cbb.find('tfoot')\n",
    "            for row in table_foot.findAll('tr'):\n",
    "                cells  = row.findAll('td')\n",
    "                playerData = str(cells) #the indexes are not uniform across the database\n",
    "                NCAA_fgpct = re.search(r'data-stat=\"fg_pct\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_games = re.search(r'data-stat=\"g\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_ppg = re.search(r'data-stat=\"pts_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_ft = re.search(r'data-stat=\"ft_pct\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_fgpg = re.search(r'data-stat=\"fg_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_fgapg = re.search(r'data-stat=\"fga_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_ftpg = re.search(r'data-stat=\"ft_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                NCAA_ftapg = re.search(r'data-stat=\"fta_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                if re.search(r'data-stat=\"fg3_per_g\">(.*?)</td>', playerData) != None:\n",
    "                    NCAA__3ptpg = re.search(r'data-stat=\"fg3_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                else:\n",
    "                    NCAA__3ptpg = None\n",
    "                if re.search(r'data-stat=\"fg3a_per_g\">(.*?)</td>', playerData) != None:\n",
    "                    NCAA__3ptapg = re.search(r'data-stat=\"fg3a_per_g\">(.*?)</td>', playerData).group(1)\n",
    "                else:\n",
    "                    NCAA__3ptapg = None\n",
    "                if re.search(r'data-stat=\"fg3_pct\">(.*?)</td>', playerData) != None:\n",
    "                    NCAA__3ptpct = re.search(r'data-stat=\"fg3_pct\">(.*?)</td>', playerData).group(1)\t\n",
    "                else:\n",
    "                    NCAA__3ptpct = None\n",
    "                if re.search(r'data-stat=\"efg_pct\">(.*?)</td>', playerData) != None:\n",
    "                    NCAA_efgpct = re.search(r'data-stat=\"efg_pct\">(.*?)</td>', playerData).group(1)\n",
    "                else:\n",
    "                    NCAA_efgpct = None\n",
    "                break\n",
    "\n",
    "    player_entry = {'NBA_fg%':fgpct ,\n",
    "                    'NBA_g_played': games,\n",
    "                    'NBA_ppg': ppg,\n",
    "                    'NBA_ft%': ft,\n",
    "                    'NBA_fg_per_game': fgpg,\n",
    "                    'NBA_fga_per_game': fgapg,\n",
    "                    'NBA_ft_per_g': ftpg,\n",
    "                    'NBA_fta_p_g': ftapg,\n",
    "                    'NBA__3ptpg': _3ptpg,\n",
    "                    'NBA__3ptapg': _3ptapg,\n",
    "                    'NBA__3ptpct': _3ptpct,\n",
    "                    'NBA_efgpct': efgpct,\n",
    "                    'NCAA_fgpct': NCAA_fgpct,\n",
    "                    'NCAA_games': NCAA_games,\n",
    "                    'NCAA_ppg' : NCAA_ppg,\n",
    "                    'NCAA_ft': NCAA_ft,\n",
    "                    'NCAA_fgpg': NCAA_fgpg,\n",
    "                    'NCAA_fgapg': NCAA_fgapg,\n",
    "                    'NCAA_ftpg': NCAA_ftpg,\n",
    "                    'NCAA_ftapg': NCAA_ftapg,\n",
    "                    'NCAA__3ptpg': NCAA__3ptpg,\n",
    "                    'NCAA__3ptapg': NCAA__3ptapg,\n",
    "                    'NCAA__3ptpct': NCAA__3ptpct,\n",
    "                    'NCAA_efgpct': NCAA_efgpct\n",
    "                    }\n",
    "    \n",
    "    #print('player ' + url + 'complete')\n",
    "    #print(player_entry)\n",
    "    return player_entry\n",
    "\n",
    "def get_player_college_url(NBA_url):\n",
    "    page_request = requests.get('http://www.basketball-reference.com' + str(url))\n",
    "    soup = BeautifulSoup(page_request.text,\"lxml\")\n",
    "    links = str(soup.findAll('li')) #regex time\n",
    "    college_url = re.search(r'<a href=\"(.*?)\">College Basketball at Sports-Reference.com</a>', links)\n",
    "    if(college_url != None):\n",
    "        return str(college_url.group(1))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "######################################################################################\n",
    "#MAIN \n",
    "players_general_info = player_basic_info() # call function that scrapes general info\n",
    "print('General info/player url loaded...')\n",
    "players_details_info_list = []\n",
    "df = pd.DataFrame()\t\n",
    "bar = progressbar.ProgressBar(max_value=len(players_general_info))\n",
    "for i,url in enumerate(players_general_info.url):\n",
    "    player = player_info(url)\n",
    "    df = df.append(player, ignore_index = True)\n",
    "    print(df)\n",
    "    bar.update(i)\n",
    "    time.sleep(0.1)\n",
    "print('Done!') #takes an unholy amount of time\n",
    "df = pd.concat([players_general_info, df], axis =1, join_axes=[df.index])\n",
    "df.to_csv('players.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "\n",
    " #print(players_details_info_list[i])\n",
    "# for i,url in enumerate(players_general_info.url):\n",
    "# \ttry:\n",
    "# \t    players_details_info_list.append(player_detail_info(url))\n",
    "# \t    print(players_details_info_list[i])\n",
    "# \texcept:\n",
    "# \t\tprint('cannot load: %s; location %d' %(url,i)) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
